# HuggingFace Local Model Configuration
# Use this to run the pipeline with local open-source models

llm_backend: huggingface

segmenter:
  strategy: pysbd
  text_column: Responses
  max_segments_per_response: 6

embedder:
  model_name: intfloat/e5-small-v2
  batch_size: 512
  normalize: true

labeler:
  max_examples_per_cluster: 40
  max_words: 5
  system_prompt: >
    You are a market research analyst. Summarize the theme of the provided survey segments.

coder:
  include_examples: true
  output_path: data/coded_responses.json

# HuggingFace model configuration
huggingface:
  # Popular options:
  # - meta-llama/Llama-3.2-3B-Instruct (good balance, requires Llama access)
  # - mistralai/Mistral-7B-Instruct-v0.2 (strong performance)
  # - microsoft/Phi-3-mini-4k-instruct (small, fast, no gating)
  # - google/gemma-2-2b-it (efficient, good quality)
  # - Qwen/Qwen2.5-3B-Instruct (multilingual support)
  model_name: microsoft/Phi-3-mini-4k-instruct
  device: auto  # or "cuda", "cpu"
  temperature: 0.3
  max_new_tokens: 768
  load_in_8bit: false  # Set true to reduce memory (requires bitsandbytes)
  load_in_4bit: false  # Set true for even lower memory (requires bitsandbytes)

storage:
  base_dir: data

discovery:
  enabled: true
  max_iterations: 2
  assignment_threshold: 0.4
  max_bootstrap_nets: 8
  bootstrap_rounds: 1
  bootstrap_seeds_per_net: 5
  refresh_sample_size: 100
  min_net_size: 15
  merge_similarity_threshold: 0.85
  unknown_label: Unknown
  max_unknown_clusters: 5
  improvement_tol: 0.01
  max_llm_calls: 30  # Keep lower for local models to save time
  deterministic: true
  duplicate_similarity_threshold: 0.85
  dedupe_enabled: true
  subnet:
    enabled: false  # Disable for faster local testing

